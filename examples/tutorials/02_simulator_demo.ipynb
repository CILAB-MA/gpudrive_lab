{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `GPUDrive` simulator concepts\n",
    "\n",
    "In this notebook, we demonstrate how to work with the `GPUDrive` simulator and access its basic attributes in Python. The simulator, written in C++, is built on top of the [Madrona Engine](https://madrona-engine.github.io/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpudrive\n",
    "import torch \n",
    "import os\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "- `GPUDrive` simulations are discretized traffic scenarios. A scenario is a constructed snapshot of traffic situation at a particular timepoint.\n",
    "- The state of the vehicle of focus is referred to as the **ego state**. Each vehicle has their own partial view of the traffic scene; and a visible state is constructed by parameterizing the view distance of the driver. The **action** for each vehicle is a (1, 3) tuple with the acceleration, steering and head angle of the vehicle.\n",
    "- The `step()` method advances the simulation with a desired step size. By default, the dynamics of vehicles are driven by a kinematic bicycle model. If a vehicle is not controlled (that is, we do not give it actions), its position, heading, and speed will be updated according to a the human expert demonstrations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiating a sim object with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim = gpudrive.SimManager(\n",
    "    exec_mode=gpudrive.madrona.ExecMode.CUDA if device == \"cuda\" else gpudrive.madrona.ExecMode.CPU, \n",
    "    gpu_id=0, \n",
    "    num_worlds=1, # The number of parallel environments\n",
    "    json_path='../example_data', # Path to data files, must be structured as above\n",
    "    params = gpudrive.Parameters() # Environment parameters\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simulator provides the following functions:\n",
    "- `reset(world_idx)` resets a specific world or environment at the given index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.reset(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `step()` advances the dynamics of all worlds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim.step() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exporting tensors\n",
    "\n",
    "To retrieve a tensor from the simulator, call the specific `tensor()` method, followed by either `to_torch()` or `to_jax()`.\n",
    "\n",
    "For example, here is how to access the ego state, or self-observation tensor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 128, 6]), device(type='cpu'))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_tensor = sim.self_observation_tensor().to_torch()\n",
    "\n",
    "observation_tensor.shape, observation_tensor.device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or alternatively:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 128, 6), {CpuDevice(id=0)})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_tensor_jax = sim.self_observation_tensor().to_jax()\n",
    "\n",
    "observation_tensor_jax.shape, observation_tensor_jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> #### ✏️ All these extractions are handled for you in the `gym` environments introduced in the next tutorial.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are all available tensor exports and methods on the sim object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "absolute_self_observation_tensor\n",
      "action_tensor\n",
      "agent_roadmap_tensor\n",
      "controlled_state_tensor\n",
      "depth_tensor\n",
      "done_tensor\n",
      "expert_trajectory_tensor\n",
      "info_tensor\n",
      "lidar_tensor\n",
      "map_observation_tensor\n",
      "partner_observations_tensor\n",
      "reset\n",
      "reset_tensor\n",
      "response_type_tensor\n",
      "reward_tensor\n",
      "rgb_tensor\n",
      "self_observation_tensor\n",
      "shape_tensor\n",
      "step\n",
      "steps_remaining_tensor\n",
      "valid_state_tensor\n"
     ]
    }
   ],
   "source": [
    "for attr in dir(sim):\n",
    "    if not attr.startswith('_'):\n",
    "        print(attr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect valid and controlled agents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check the number of agents and road points in each world, you can use the `shape_tensor`.\n",
    "\n",
    "The shape tensor is a 2D tensor where the first dimension represents the number of worlds, and the second dimension represents the shape of each world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape tensor has a shape of (Num Worlds, 2): (1, 2)\n",
      "World 0 has 10 VALID agents and 3195 VALID road objects\n"
     ]
    }
   ],
   "source": [
    "shape_tensor = sim.shape_tensor().to_jax()\n",
    "print(f'Shape tensor has a shape of (Num Worlds, 2): {shape_tensor.shape}')\n",
    "\n",
    "for world_idx in range(shape_tensor.shape[0]):\n",
    "    print(f'World {world_idx} has {shape_tensor[world_idx][0]} VALID agents and {shape_tensor[world_idx][1]} VALID road objects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of **valid** agents indicates the number of controllable agents (vehicles). Some vehicles or bicycles may be initialized in incorrect positions or remain static; these are marked as **invalid** and cannot be controlled.\n",
    "\n",
    "The sim comes with a mask that indicates which agents can be controlled. Entries are `1` for agents that can be controlled, and `0` otherwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Controlled state tensor has a shape of (num_worlds, max_num_objects, 1):  torch.Size([1, 128, 1])\n"
     ]
    }
   ],
   "source": [
    "controlled_state_tensor = sim.controlled_state_tensor().to_torch()\n",
    "print(\"Controlled state tensor has a shape of (num_worlds, max_num_objects, 1): \", controlled_state_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can control 3 agents in this world\n",
    "controlled_state_tensor.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "controlled_state_tensor.sum().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action tensor stores the current actions for all agents across all worlds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action tensor has a shape of (num_worlds, max_num_objects, 3): torch.Size([1, 128, 3])\n"
     ]
    }
   ],
   "source": [
    "action_tensor = sim.action_tensor().to_torch()\n",
    "print(f'Action tensor has a shape of (num_worlds, max_num_objects, 3): {action_tensor.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the actions for all controlled agents, we use the `copy_()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions tensor after setting all actions to 1: tensor([1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "actions_tensor = sim.action_tensor().to_torch()\n",
    "\n",
    "actions = torch.full(actions_tensor.shape, 1.0)\n",
    "actions_tensor.copy_(actions)\n",
    "\n",
    "print(f'Actions tensor after setting all actions to 1: {actions_tensor[0][0]}')\n",
    "\n",
    "# Call step() to apply the actions\n",
    "sim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting the simulator settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "IgnoreNonVehicles   : False\n",
      "collisionBehaviour  : gpudrive.CollisionBehaviour.AgentStop\n",
      "datasetInitOptions  : gpudrive.DatasetInitOptions.FirstN\n",
      "disableClassicalObs : False\n",
      "enableLidar         : False\n",
      "initOnlyValidAgentsAtFirstStep : True\n",
      "isStaticAgentControlled: False\n",
      "maxNumControlledVehicles: 10000\n",
      "observationRadius   : 0.0\n",
      "polylineReductionThreshold: 0.0\n",
      "rewardParams        : <gpudrive.RewardParams object at 0x7f0393f610b0>\n",
      "Reward parameters:\n",
      "    distanceToExpertThreshold: 0.0\n",
      "    distanceToGoalThreshold: 0.0\n",
      "    rewardType        : gpudrive.RewardType.DistanceBased\n",
      "roadObservationAlgorithm: gpudrive.FindRoadObservationsWith.KNearestEntitiesWithRadiusFiltering\n",
      "useWayMaxModel      : False\n"
     ]
    }
   ],
   "source": [
    "params = gpudrive.Parameters()\n",
    "\n",
    "print('Parameters:')\n",
    "for attr in dir(params):\n",
    "    if not attr.startswith('__'):\n",
    "        value = getattr(params, attr)\n",
    "        print(f\"{attr:20}: {value}\")\n",
    "        if attr == 'rewardParams':\n",
    "            print('Reward parameters:')\n",
    "            reward_params = getattr(params, attr)\n",
    "            for attr2 in dir(reward_params):\n",
    "                if not attr2.startswith('__'):\n",
    "                    value2 = getattr(reward_params, attr2)\n",
    "                    print(f\"    {attr2:18}: {value2}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting the simulator parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To set the parameters of the simulator, fill in the values for each attribute of the parameter object as below. This allows you to customize the simulation settings.\n",
    "\n",
    "The params object can be passed to the sim constructor like this:\n",
    "\n",
    "```Python\n",
    "sim = gpudrive.SimManager(\n",
    "    ...\n",
    "    params=params \n",
    ")\n",
    "```\n",
    "\n",
    "See our [README](https://github.com/Emerge-Lab/gpudrive/tree/main?tab=readme-ov-file#configuring-the-sim) for the full documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_params = gpudrive.RewardParams()\n",
    "reward_params.rewardType = gpudrive.RewardType.DistanceBased\n",
    "reward_params.distanceToGoalThreshold = 1.0\n",
    "reward_params.distanceToExpertThreshold = 1.0 \n",
    "\n",
    "# Initialize Parameters\n",
    "params = gpudrive.Parameters()\n",
    "params.polylineReductionThreshold = 1.0\n",
    "params.observationRadius = 100.0\n",
    "params.datasetInitOptions = gpudrive.DatasetInitOptions.RandomN\n",
    "params.collisionBehaviour = gpudrive.CollisionBehaviour.Ignore\n",
    "params.maxNumControlledVehicles = 10\n",
    "params.rewardParams = reward_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an episode of the sim\n",
    "\n",
    "Putting everything together, the full interaction loop looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_worlds = 1\n",
    "\n",
    "for i in range(num_worlds):\n",
    "    sim.reset(i)\n",
    "\n",
    "actions_shape = sim.action_tensor().to_torch().shape\n",
    "dones = sim.done_tensor().to_torch()\n",
    "while not torch.all(sim.done_tensor().to_torch()):\n",
    "    obs, rews, dones = sim.self_observation_tensor().to_torch(), sim.reward_tensor().to_torch(), sim.done_tensor().to_torch()\n",
    "    actions = torch.rand(actions_shape)\n",
    "    sim.action_tensor().to_torch().copy_(actions)\n",
    "    sim.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "madrona",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
